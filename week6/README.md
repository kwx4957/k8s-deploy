### kubespray offline 설치
> Cloudnet@ k8s Deploy 6주차 스터디를 진행하며 정리한 글입니다.

서버 구성

| NAME | Description | CPU | RAM | NIC1 | NIC2 | Init Script |
| --- | --- | --- | --- | --- | --- | --- |
| **admin** | Bastion, kubespray 실행 | 4 | 4GB | 10.0.2.15 | **192.168.10.10** | admin.sh |
| **k8s-node1** | K8S ControlPlane | 4 | 2GB | 10.0.2.15 | **192.168.10.11** | init-cfg.sh |
| **k8s-node2** | K8S Worker | 4 | 2GB | 10.0.2.15 | **192.168.10.12** | init-cfg.sh |

### 사전 확인
```sh
vagrant up
vagrant destory -f && rm -rf .vagrant

vagrant ssh admin

# ip 확인
ip -c -br -4 addr
lo               UNKNOWN        127.0.0.1/8 
enp0s8           UP             10.0.2.15/24 
enp0s9           UP             192.168.10.10/24 

# dns 확인
cat /etc/hosts 
192.168.10.10 admin
192.168.10.11 k8s-node1
192.168.10.12 k8s-node2
```


### 폐쇄망을 위한 기본 설정
```sh
# k8s-node
ssh root@k8s-node1

# 인터넷 통신 테스트
ping -c 1 8.8.8.8

# 기본 라우팅 제거, 외부 통신 안됨
nmcli connection down enp0s8
nmcli connection modify enp0s8 connection.autoconnect no
nmcli connection modify enp0s9 +ipv4.routes "0.0.0.0/0 192.168.10.10 200"
nmcli connection up enp0s9

# 인터페이스 설정 확인
cat /etc/NetworkManager/system-connections/enp0s8.nmconnection
cat /etc/NetworkManager/system-connections/enp0s9.nmconnection

# 라우팅 확인
ip route
default via 192.168.10.10 dev enp0s8 proto static metric 200
192.168.10.0/24 dev enp0s8 proto kernel scope link src 192.168.10.11 metric 100

# 통신 X
ping -w 1 -W 1 8.8.8.8
1 packets transmitted, 0 received, 100% packet loss, time 0ms

curl www.google.com
curl: (6) Could not resolve host: www.google.com

cat /etc/resolv.conf
# Generated by NetworkManager

# dns 추가
cat << EOF > /etc/resolv.conf
nameserver 168.126.63.1
nameserver 8.8.8.8
EOF

# 통신 X
curl www.google.com

exit

# admin 서버 포워딩 설정
sysctl -w net.ipv4.ip_forward=1 
cat <<EOF | tee /etc/sysctl.d/99-ipforward.conf
net.ipv4.ip_forward = 1
EOF

sysctl --system
net.ipv4.ip_forward = 1

# nat 조회
iptables -t nat -A POSTROUTING -o enp0s8 -j MASQUERADE

iptables -t nat -S
-P PREROUTING ACCEPT
-P INPUT ACCEPT
-P OUTPUT ACCEPT
-P POSTROUTING ACCEPT
-A POSTROUTING -o enp0s8 -j MASQUERADE

iptables -t nat -L -n -v
Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
    1    76 MASQUERADE  all  --  *      enp0s3  0.0.0.0/0            0.0.0.0/0

ssh root@k8s-node1

# 인터넷 통신 테스트
ping -c 1 8.8.8.8
1 packets transmitted, 1 received, 0% packet loss, time 0ms

exit

# 폐쇄망을 만들기 위한 nat 정책 제거
iptables -t nat -D POSTROUTING -o enp0s8 -j MASQUERADE

### NTP
# ntp 확인
systemctl status chronyd.service --no-pager
● chronyd.service - NTP client/server
     Loaded: loaded (/usr/lib/systemd/system/chronyd.service; enabled; preset: enabled)
     Active: active (running) since Sat 2026-02-14 17:27:46 KST; 19min ago
 Invocation: 018bd5f3a688475a8e9b666fb36a4ad6
       Docs: man:chronyd(8)
             man:chrony.conf(5)
   Main PID: 650 (chronyd)
      Tasks: 1 (limit: 12336)
     Memory: 2.8M (peak: 5.4M, swap: 216K, swap peak: 216K)
        CPU: 63ms
     CGroup: /system.slice/chronyd.service
             └─650 /usr/sbin/chronyd -F 2

# ntp 동기화 서버 
grep "^[^#]" /etc/chrony.conf
pool 2.rocky.pool.ntp.org iburst
sourcedir /run/chrony-dhcp
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
ntsdumpdir /var/lib/chrony
logdir /var/log/chrony

# ntp의 동기화 서버 목록 및 기준 확인
chronyc sources -v
MS Name/IP address         Stratum Poll Reach LastRx Last sample               
===============================================================================
^* 121.134.215.104               2   6   377    53   -108us[  -29ms] +/- 2847us
^- mail.zeroweb.kr               3   6   377    53   +335us[  -28ms] +/-   47ms
^- 203.32.26.46                  2   6   377    53    +13ms[  +13ms] +/-  180ms
^- ec2-3-39-176-65.ap-north>     2   6   277    51   +125us[ +125us] +/- 5932us

dig +short 2.rocky.pool.ntp.org
175.210.18.47
193.123.243.2
158.247.202.103

cp /etc/chrony.conf /etc/chrony.bak

# ntp 동기화 서버 설정 및 허용 대역대 설정
cat << EOF > /etc/chrony.conf
server pool.ntp.org iburst
server kr.pool.ntp.org iburst

allow 192.168.10.0/24

local stratum 10

logdir /var/log/chrony
EOF

systemctl restart chronyd.service

systemctl status chronyd.service --no-pager
Active: active (running) since Sat 2026-02-14 17:50:08 KST; 5s ago

timedatectl status
               Local time: Sat 2026-02-14 17:50:30 KST
           Universal time: Sat 2026-02-14 08:50:30 UTC
                 RTC time: Sat 2026-02-14 09:12:49
                Time zone: Asia/Seoul (KST, +0900)
System clock synchronized: yes
              NTP service: active
          RTC in local TZ: no

chronyc sources -v
^- 121.134.215.104               2   6    17    36   +265us[ +265us] +/- 3416us
^* 211.108.117.211               2   6    17    37    +64us[+1009us] +/- 2601us

dig +short  pool.ntp.org
121.174.142.82
121.174.142.81
121.134.215.104
211.108.117.211

# k8s 노드 ntp 동기화 서버 설정
ssh root@k8s-node1

timedatectl status
               Local time: Sat 2026-02-14 17:52:38 KST
           Universal time: Sat 2026-02-14 08:52:38 UTC
                 RTC time: Sat 2026-02-14 09:14:56
                Time zone: Asia/Seoul (KST, +0900)
System clock synchronized: yes
              NTP service: active
          RTC in local TZ: no

# 현재 ntp 서버 대상 확인
chronyc sources -v

cp /etc/chrony.conf /etc/chrony.bak

# admin 서버를 동기화 서버 대상으로 지정
cat << EOF > /etc/chrony.conf
server 192.168.10.10 iburst
logdir /var/log/chrony
EOF

systemctl restart chronyd.service

systemctl status chronyd.service --no-pager
     Active: active (running) since Sat 2026-02-14 17:54:00 KST; 6s ago
Feb 14 17:54:00 k8s-node1 systemd[1]: Started chronyd.service - NTP client/server.
Feb 14 17:54:05 k8s-node1 chronyd[5647]: Selected source 192.168.10.10
Hint: Some lines were ellipsized, use -l to show in full.


timedatectl status
               Local time: Sat 2026-02-14 17:54:27 KST
           Universal time: Sat 2026-02-14 08:54:27 UTC
                 RTC time: Sat 2026-02-14 09:16:46
                Time zone: Asia/Seoul (KST, +0900)
System clock synchronized: yes
              NTP service: active
          RTC in local TZ: no

chronyc sources -v
^* admin                         3   6    17    33  -1614ns[  +15us] +/- 5445us

exit

chronyc clients
Hostname                      NTP   Drop Int IntL Last     Cmd   Drop Int  Last
===============================================================================
k8s-node1                       4      0   1   -    53       0      0   -     -
k8s-node2                       1      0   -   -    23       0      0   -     -

### dns
# admin
dnf install -y bind bind-utils

cp /etc/named.conf /etc/named.bak

cat <<EOF > /etc/named.conf
options {
        listen-on port 53 { any; };
        listen-on-v6 port 53 { ::1; };
        directory       "/var/named";
        dump-file       "/var/named/data/cache_dump.db";
        statistics-file "/var/named/data/named_stats.txt";
        memstatistics-file "/var/named/data/named_mem_stats.txt";
        secroots-file   "/var/named/data/named.secroots";
        recursing-file  "/var/named/data/named.recursing";
        allow-query     { 127.0.0.1; 192.168.10.0/24; };
        allow-recursion { 127.0.0.1; 192.168.10.0/24; };

        forwarders {
                168.126.63.1;
                8.8.8.8;
        };

        recursion yes;

        dnssec-validation auto;  # https://sirzzang.github.io/kubernetes/Kubernetes-Kubespray-08-01-06/#troubleshooting-dnssec-%EA%B2%80%EC%A6%9D-%EC%8B%A4%ED%8C%A8

        managed-keys-directory "/var/named/dynamic";
        geoip-directory "/usr/share/GeoIP";

        pid-file "/run/named/named.pid";
        session-keyfile "/run/named/session.key";

        include "/etc/crypto-policies/back-ends/bind.config";
};

logging {
        channel default_debug {
                file "data/named.run";
                severity dynamic;
        };
};

zone "." IN {
        type hint;
        file "named.ca";
};

include "/etc/named.rfc1912.zones";
include "/etc/named.root.key";
EOF

# 설정 오류 확인
named-checkconf /etc/named.conf

# dns 시작
systemctl enable --now named

# 자기 자신 질의 
cat /etc/resolv.conf
echo "nameserver 192.168.10.10" > /etc/resolv.conf

dig +short google.com @192.168.10.10
142.250.199.14

dig +short google.com
142.250.199.14

cat /etc/NetworkManager/conf.d/99-dns-none.conf
cat: /etc/NetworkManager/conf.d/99-dns-none.conf: No such file or directory

# networkmanager의 dns 관리 비활성화
cat << EOF > /etc/NetworkManager/conf.d/99-dns-none.conf
[main]
dns=none
EOF
systemctl restart NetworkManager

# k8s-node가 admin dns 질의 허용하도록하기
ssh root@k8s-node1
cat /etc/NetworkManager/conf.d/99-dns-none.conf
cat: /etc/NetworkManager/conf.d/99-dns-none.conf: No such file or directory

# networkmanager의 dns 관리 비활성화
cat << EOF > /etc/NetworkManager/conf.d/99-dns-none.conf
[main]
dns=none
EOF

systemctl restart NetworkManager
echo "nameserver 192.168.10.10" > /etc/resolv.conf

dig +short google.com @192.168.10.10
dig +short google.com

exit

### dnf repo
dnf install -y dnf-plugins-core createrepo nginx
mkdir -p /data/repos/rocky/10
cd /data/repos/rocky/10
dnf repolist

dnf reposync --repoid=baseos    --download-metadata -p /data/repos/rocky/10
du -sh /data/repos/rocky/10/baseos/
ls -l /data/repos/rocky/10/baseos/repodata/
cat /data/repos/rocky/10/baseos/repodata/repomd.xml 
dnf reposync --repoid=appstream --download-metadata -p /data/repos/rocky/10
du -sh /data/repos/rocky/10/appstream/
dnf reposync --repoid=extras    --download-metadata -p /data/repos/rocky/10
du -sh /data/repos/rocky/10/extras/

cat <<EOF > /etc/nginx/conf.d/repos.conf
server {
    listen 80;
    server_name repo-server;

    location /rocky/10/ {
        autoindex on;                 
        autoindex_exact_size off;     
        autoindex_localtime on;       
        root /data/repos;
    }
}
EOF

# nginx 시작 
systemctl enable --now nginx
systemctl status nginx.service --no-pager
ss -tnlp | grep nginx

# 접속 테스트
curl http://192.168.10.10/rocky/10/

# k8s-node
ssh root@k8s-node1

tree /etc/yum.repos.d/

mkdir /etc/yum.repos.d/backup
mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup/

# nginx 웹서버가 제공하는 레포지토리 참조하도록 설정
cat <<EOF > /etc/yum.repos.d/internal-rocky.repo
[internal-baseos]
name=Internal Rocky 10 BaseOS
baseurl=http://192.168.10.10/rocky/10/baseos
enabled=1
gpgcheck=0

[internal-appstream]
name=Internal Rocky 10 AppStream
baseurl=http://192.168.10.10/rocky/10/appstream
enabled=1
gpgcheck=0

[internal-extras]
name=Internal Rocky 10 Extras
baseurl=http://192.168.10.10/rocky/10/extras
enabled=1
gpgcheck=0
EOF

# 캐시 초기화 후 업데이트 
dnf clean all
dnf repolist
dnf makecache

dnf install -y nfs-utils

dnf info nfs-utils | grep -i repo
Repository   : @System
From repo    : internal-baseos

# admin
# 다음 실습을 위해서 삭제
systemctl disable --now nginx && dnf remove -y nginx
```

### Admin 서버 offline 사전 작업 [^1]
```sh
# 가용용량 확인
lsblk
sda      8:0    0   120G  0 disk 
├─sda1   8:1    0   600M  0 part /boot/efi
├─sda2   8:2    0   3.8G  0 part [SWAP]
└─sda3   8:3    0 115.6G  0 part /

df -hT /
Filesystem     Type  Size  Used Avail Use% Mounted on
/dev/sda3      xfs   117G  4.8G  113G   5% /

git clone https://github.com/kubespray-offline/kubespray-offline
tree kubespray-offline/
cd kubespray-offline/

# 환경 변수 설정
# k8s 관련 컴포넌트가 정의되어 있다. config.sh -> target-scripts/config.sh 
source ./config.sh
echo -e "kubespary $KUBESPRAY_VERSION"
echo -e "runc $RUNC_VERSION"
echo -e "containerd $CONTAINERD_VERSION"
echo -e "nercdtl $NERDCTL_VERSION"
echo -e "cni $CNI_VERSION"
echo -e "nginx $NGINX_VERSION"
echo -e "registry $REGISTRY_VERSION"
echo -e "registry_port: $REGISTRY_PORT"
echo -e "Additional container registry hosts: $ADDITIONAL_CONTAINER_REGISTRY_LIST"
echo -e "cpu arch: $IMAGE_ARCH"

# 파일을 읽으면 내부적으로 sh를 참조하고 있다.
cat download-all.sh

# 모든 의존성 설치
./download-all.sh
(230/230): libxml2-2.12.5-9.el10_0.x86_64.rpm                                                           9.6 MB/s | 691 kB     00:00     
/bin/rm: cannot remove 'outputs/rpms/local/*.i686.rpm': No such file or directory
==> createrepo
Directory walk started
Directory walk done - 230 packages
Temporary output repo path: outputs/rpms/local/.repodata/
Pool started (with 5 workers)
Pool finished
create-repo done.
=> Running: ./copy-target-scripts.sh
==> Copy target scripts
Done.

du -sh ~/.venv
491M    /root/.venv

tree ~/.venv | more
tree ~/.cache | more

du -sh ~/.cache
819M    /root/.cache

tree /root/kubespray-offline/cache/kubespray-2.30.0/contrib/offline/
/root/kubespray-offline/cache/kubespray-2.30.0/contrib/offline/
├── docker-daemon.json
├── generate_list.sh
├── generate_list.yml
├── manage-offline-container-images.sh
├── manage-offline-files.sh
├── nginx.conf
├── README.md
├── registries.conf
├── temp
│   ├── files.list
│   ├── files.list.template
│   ├── images.list
│   └── images.list.template
└── upload2artifactory.py
2 directories, 13 files

du -sh /root/kubespray-offline/outputs/
3.7G    /root/kubespray-offline/outputs/

tree /root/kubespray-offline/outputs/ | more

tree /root/kubespray-offline/outputs/ -L 1
/root/kubespray-offline/outputs/
├── config.sh
├── config.toml
├── containerd.service
├── extract-kubespray.sh
├── files
├── images
├── install-containerd.sh
├── load-push-all-images.sh
├── nginx-default.conf
├── patches
├── playbook
├── pypi
├── pyver.sh
├── rpms
├── setup-all.sh
├── setup-container.sh
├── setup-offline.sh
├── setup-py.sh
├── start-nginx.sh
├── start-registry.sh
└── venv.sh
7 directories, 15 files

cd /root/kubespray-offline/outputs
ls -l *.sh
-rw-r--r--. 1 root root 1371 Feb 10 12:25 config.sh
-rwxr-xr-x. 1 root root  719 Feb 10 12:25 extract-kubespray.sh
-rwxr-xr-x. 1 root root 2544 Feb 10 12:25 install-containerd.sh
-rwxr-xr-x. 1 root root 1141 Feb 10 12:25 load-push-all-images.sh
-rw-r--r--. 1 root root  607 Feb 10 12:25 pyver.sh
-rwxr-xr-x. 1 root root  394 Feb 10 12:25 setup-all.sh
-rwxr-xr-x. 1 root root  408 Feb 10 12:25 setup-container.sh
-rwxr-xr-x. 1 root root 2106 Feb 10 12:25 setup-offline.sh
-rwxr-xr-x. 1 root root 1213 Feb 10 12:25 setup-py.sh
-rwxr-xr-x. 1 root root  654 Feb 10 12:25 start-nginx.sh
-rwxr-xr-x. 1 root root  445 Feb 10 12:25 start-registry.sh
-rw-r--r--. 1 root root  322 Feb 10 12:25 venv.sh

# 컨테이너d 설치 후 nginx와 레지스트리 이미지 불러오기
./setup-container.sh

which runc && runc --version
/usr/local/bin/runc
runc version 1.3.4

which containerd && containerd --version
/usr/local/bin/containerd
containerd github.com/containerd/containerd/v2 v2.2.1 dea7da592f5d1d2b7755e3a161be07f43fad8f75

which nerdctl && nerdctl --version
/usr/local/bin/nerdctl
nerdctl version 2.2.1

tree -ug /opt/cni/bin/
[root     root    ]  /opt/cni/bin/
├── [root     root    ]  bandwidth
├── [root     root    ]  bridge
├── [root     root    ]  dhcp
├── [root     root    ]  dummy
├── [root     root    ]  firewall
├── [root     root    ]  host-device
├── [root     root    ]  host-local
├── [root     root    ]  ipvlan
├── [root     root    ]  LICENSE
├── [root     root    ]  loopback
├── [root     root    ]  macvlan
├── [root     root    ]  portmap
├── [root     root    ]  ptp
├── [root     root    ]  README.md
├── [root     root    ]  sbr
├── [root     root    ]  static
├── [root     root    ]  tap
├── [root     root    ]  tuning
├── [root     root    ]  vlan
└── [root     root    ]  vrf


cat /etc/containerd/config.toml
cat /etc/systemd/system/containerd.service
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target local-fs.target
[Service]
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/usr/local/bin/containerd
Type=notify
Delegate=yes
KillMode=process
Restart=always
RestartSec=5
LimitNPROC=infinity
LimitCORE=infinity
LimitNOFILE=infinity
TasksMax=infinity
OOMScoreAdjust=-999
[Install]
WantedBy=multi-user.target

systemctl status containerd.service --no-pager

nerdctl images
REPOSITORY    TAG              IMAGE ID        CREATED               PLATFORM       SIZE       BLOB SIZE
nginx         1.29.4           93c49ce72e03    About a minute ago    linux/amd64    171MB      164.3MB
nginx         1.28.0-alpine    dc8e6d3967a0    About a minute ago    linux/amd64    51.18MB    49.69MB
registry      3.0.0            09d6d68c85b9    About a minute ago    linux/amd64    58.44MB    58.26MB
registry      2.8.1            1e6c7d1be0dd    About a minute ago    linux/amd64    26.65MB    26.49MB

# nginx 설정 
cp nginx-default.conf nginx-default.bak
cat << EOF > nginx-default.conf 
server {
    listen       80;
    listen  [::]:80;
    server_name  localhost;

    location / {
        root   /usr/share/nginx/html;
        # index  index.html index.htm;

        autoindex on;                
        autoindex_exact_size off;     
        autoindex_localtime on;       
    }

    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }

    # Force sendfile to off
    sendfile off;     
}
EOF

# nginx 컨테이너 실행, 현재 경로의 nginx 설정을 불러온다
./start-nginx.sh

nerdctl ps
CONTAINER ID    IMAGE                             COMMAND                   CREATED          STATUS    PORTS    NAMES
cb5c3249097b    docker.io/library/nginx:1.29.4    "/docker-entrypoint.…"    5 seconds ago    Up                 
nginx

ss -tnlp | grep nginx
LISTEN 0      511          0.0.0.0:80         0.0.0.0:*    users:(("nginx",pid=20557,fd=6),("nginx",pid=20556,fd=6),("nginx",pid=20555,fd=6),("nginx",pid=20554,fd=6),("nginx",pid=20521,fd=6))
LISTEN 0      511             [::]:80            [::]:*    users:(("nginx",pid=20557,fd=7),("nginx",pid=20556,fd=7),("nginx",pid=20555,fd=7),("nginx",pid=20554,fd=7),("nginx",pid=20521,fd=7))

# nginx가 웹서버 역할로 가져온 바이너리등 설치파일등을 서빙한다.
open http://192.168.10.10/

dnf repolist
repo id                                         repo name
appstream                                       Rocky Linux 10 - AppStream
baseos                                          Rocky Linux 10 - BaseOS
extras                                          Rocky Linux 10 - Extras

cat /etc/redhat-release
Rocky Linux release 10.0 (Red Quartz)

# 환경변수 설정
# 레포지토리 비활성화, pypi 미러 설정
./setup-offline.sh
/bin/rm: cannot remove '/etc/yum.repos.d/offline.repo': No such file or directory
===> Disable all yumrepositories
===> Setup local yum repository
[offline-repo]
name=Offline repo
baseurl=http://localhost/rpms/local/
enabled=1
gpgcheck=0
===> Setup PyPI mirror

cat /etc/yum.repos.d/offline.repo
[offline-repo]
name=Offline repo
baseurl=http://localhost/rpms/local/
enabled=1
gpgcheck=0

dnf clean all
18 files removed

dnf repolist
repo id                                                 repo name
offline-repo                                            Offline repo

cat ~/.config/pip/pip.conf
[global]
index = http://localhost/pypi/
index-url = http://localhost/pypi/
trusted-host = localhost

# 파이썬 설치
./setup-py.sh
===> Install python, venv, etc
Offline repo                                                                     21 MB/s |  85 kB     00:00    
Package python3-3.12.12-3.el10_1.x86_64 is already installed.
Dependencies resolved.
Nothing to do.
Complete!
root@admin:~/kube

source pyver.sh

echo -e "python_version $python${PY}"
python_version 3.12

dnf info python3
Source       : python3.12-3.12.12-3.el10_1.src.rpm
Repository   : @System
From repo    : baseos
Summary      : Python 3.12 interpreter
URL          : https://www.python.org/
License      : Python-2.0.1

tree rpms/local/ | grep -i python | wc -l
23

# 로컬 레지스트리 실행
# /var/lib/registry 경로 마운트
./start-registry.sh
===> Start registry
4b5a42d82715d780f4f94e7cf50d16bee09f96e0ac9c8c8f275e9973457e496d

source config.sh
echo -e "registry_port: $REGISTRY_PORT"

nerdctl ps
CONTAINER ID    IMAGE                               COMMAND                   CREATED           STATUS    PORTS    NAMES
4b5a42d82715    docker.io/library/registry:3.0.0    "/entrypoint.sh /etc…"    31 seconds ago    Up              
   registry
cb5c3249097b    docker.io/library/nginx:1.29.4      "/docker-entrypoint.…"    4 minutes ago     Up              
   nginx

ss -tnlp | grep registry
LISTEN 0      4096               *:5001             *:*    users:(("registry",pid=20678,fd=6))                  

LISTEN 0      4096               *:35000            *:*    users:(("registry",pid=20678,fd=3))                  

tree /var/lib/registry/
/var/lib/registry/
0 directories, 0 files

# 메트릭 및 프로파일링 정보 확인
curl 192.168.10.10:5001/metrics
curl 192.168.10.10:5001/debug/pprof/

# 아키텍처 확인
echo -e "cpu arch: $IMAGE_ARCH"
cpu arch: arm64

# registry.k8s.io, k8s.gcr.io, gcr.io, ghcr.io, docker.io, quay.io 외 레지스트리 추가 가능
echo -e "Additional container registry hosts: $ADDITIONAL_CONTAINER_REGISTRY_LIST"
Additional container registry hosts: myregistry.io

# 이미지 불러드릴 .tar.gz 확인
ls -l images/*.tar.gz | wc -l 
55

# arm64이라면 --all-platforms 추가
vi ./load-push-all-images.sh
load_images() {
    for image in $BASEDIR/images/*.tar.gz; do
        echo "===> Loading $image"
        sudo $NERDCTL load --all-platforms -i $image || exit 1
    done
}

# 로컬 이미지들을 이미지 레지스트리에 업로드 
./load-push-all-images.sh

nerdctl images | wc -l
111

#
nerdctl images | grep -i kube-proxy
localhost:35000/kube-proxy                               v1.34.3                                                
         fbe99026b627    40 minutes ago    linux/amd64    75.24MB    73.14MB
registry.k8s.io/kube-proxy                               v1.34.3                                                
         fbe99026b627    42 minutes ago    linux/amd64    75.24MB    73.14MB

# localhost가 포함된 이미지
nerdctl images | grep localhost | wc -l
55

# localhost가 포함되지 않은 이미지
nerdctl images | grep -v localhost | wc -l
56

# 이미지 카탈로그 확인 
curl -s http://localhost:35000/v2/_catalog | jq

# api-server 이미지 태그 확인
curl -s http://localhost:35000/v2/kube-apiserver/tags
404 page not found

# 1.34
curl -s http://localhost:35000/v2/kube-apiserver/tags/list | jq
{
  "name": "kube-apiserver",
  "tags": [
    "v1.34.3"
  ]
}

# 이미지 정보 확인
curl -s http://localhost:35000/v2/kube-apiserver/manifests/v1.34.3 | jq
{
  "schemaVersion": 2,
  "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
  "config": {
    "mediaType": "application/vnd.docker.container.image.v1+json",
    "digest": "sha256:cf65ae6c8f700cc27f57b7305c6e2b71276a7eed943c559a0091e1e667169896",
    "size": 2906
  },
  "layers": [
    {
      "mediaType": "application/vnd.docker.image.rootfs.diff.tar",
      "digest": "sha256:378b3db0974f7a5a8767b6329ad310983bc712d0e400ff5faa294f95f869cc8c",
      "size": 327680
    },
    {
      "mediaType": "application/vnd.docker.image.rootfs.diff.tar",
      "digest": "sha256:8fa10c0194df9b7c054c90dbe482585f768a54428fc90a5b78a0066a123b1bba",
      "size": 40960
    },
...

# 이미지 저장소 
tree /var/lib/registry/ -L 5
/var/lib/registry/
└── docker
    └── registry
        └── v2
            ├── blobs
            │   └── sha256
            └── repositories
                ├── amazon
                ├── calico
                ├── cilium
                ├── cloudnativelabs
                ├── coredns
                ├── coreos
                ├── cpa
                ├── dns
                ├── flannel
                ├── ingress-nginx
                ├── jetstack
                ├── k8snetworkplumbingwg
                ├── kube-apiserver
                ├── kube-controller-manager
                ├── kubeovn
                ├── kube-proxy
                ├── kubernetesui
                ├── kube-scheduler
                ├── kube-vip
                ├── library
                ├── metallb
                ├── metrics-server
                ├── mirantis
                ├── pause
                ├── provider-os
                ├── rancher
                └── sig-storage
34 directories, 0 files

# kubespray 레포에 대한 압축 파일
ls -lh files/kubespray-*
-rw-r--r--. 1 root root 2.5M Feb 14 17:34 files/kubespray-2.30.0.tar.gz

tree patches/
patches/
└── 2.18.0
    ├── 0001-nerdctl-insecure-registry-config-8339.patch
    ├── 0002-Update-config.toml.j2-8340.patch
    └── 0003-generate-list-8537.patch
2 directories, 3 files

# kubespray 압축해제 
./extract-kubespray.sh

# kubespray와 동일하다
tree kubespray-2.30.0/ -L 1
```

### kubespray 설치 [^2]
```sh
python --version
Python 3.12.12

# 가상환경 실행
python3.12 -m venv ~/.venv/3.12
source ~/.venv/3.12/bin/activate

which ansible
/root/.venv/3.12/bin/ansible

tree ~/.venv/3.12/ -L 4
42 directories, 30 files

cd /root/kubespray-offline/outputs/kubespray-2.30.0

cp ../../offline.yml .

# 인벤토리 복사
cp -r inventory/sample inventory/mycluster

# 익숙한 설정들이다.
tree inventory/mycluster/
inventory/mycluster/
├── group_vars
│   ├── all
│   │   ├── all.yml
│   │   ├── aws.yml
│   │   ├── azure.yml
│   │   ├── containerd.yml
│   │   ├── coreos.yml
│   │   ├── cri-o.yml
│   │   ├── docker.yml
│   │   ├── etcd.yml
│   │   ├── gcp.yml
│   │   ├── hcloud.yml
│   │   ├── huaweicloud.yml
│   │   ├── oci.yml
│   │   ├── offline.yml
│   │   ├── openstack.yml
│   │   ├── upcloud.yml
│   │   └── vsphere.yml
│   └── k8s_cluster
│       ├── addons.yml
│       ├── k8s-cluster.yml
│       ├── k8s-net-calico.yml
│       ├── k8s-net-cilium.yml
│       ├── k8s-net-custom-cni.yml
│       ├── k8s-net-flannel.yml
│       ├── k8s-net-kube-ovn.yml
│       ├── k8s-net-kube-router.yml
│       ├── k8s-net-macvlan.yml
│       └── kube_control_plane.yml
└── inventory.ini
4 directories, 27 files

cat offline.yml

# 참조 호스트 adminIP로 변경
# k8s에 배포에 필요한 의존성을 참조한다.
sed -i "s/YOUR_HOST/192.168.10.10/g" offline.yml

cat offline.yml | grep 192.168.10.10
http_server: "http://192.168.10.10"
registry_host: "192.168.10.10:35000"

\cp -f offline.yml inventory/mycluster/group_vars/all/offline.yml
cat inventory/mycluster/group_vars/all/offline.yml

# 인벤토리 작성
cat <<EOF > inventory/mycluster/inventory.ini
[kube_control_plane]
k8s-node1 ansible_host=192.168.10.11 ip=192.168.10.11 etcd_member_name=etcd1

[etcd:children]
kube_control_plane

[kube_node]
k8s-node2 ansible_host=192.168.10.12 ip=192.168.10.12
EOF

cat inventory/mycluster/inventory.ini

# ping 테스트 
ansible -i inventory/mycluster/inventory.ini all -m ping
k8s-node1 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.12"
    },
    "changed": false,
    "ping": "pong"
}
k8s-node2 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.12"
    },
    "changed": false,
    "ping": "pong"
}

tree ../playbook/
../playbook/
├── offline-repo.yml
└── roles
    └── offline-repo
        ├── defaults
        │   └── main.yml
        ├── files
        │   └── 99offline
        └── tasks
            ├── Debian.yml
            ├── main.yml
            └── RedHat.yml

6 directories, 6 files

mkdir offline-repo
cp -r ../playbook/ offline-repo/
tree offline-repo/
offline-repo/
└── playbook
    ├── offline-repo.yml
    └── roles
        └── offline-repo
            ├── defaults
            │   └── main.yml
            ├── files
            │   └── 99offline
            └── tasks
                ├── Debian.yml
                ├── main.yml
                └── RedHat.yml

7 directories, 6 files

# 배포
ansible-playbook -i inventory/mycluster/inventory.ini offline-repo/playbook/offline-repo.yml

# offline 레포가 이를 해결한다. 
ssh k8s-node1 tree /etc/yum.repos.d/
/etc/yum.repos.d/
├── offline.repo
├── rocky-addons.repo
├── rocky-devel.repo
├── rocky-extras.repo
└── rocky.repo

1 directory, 5 files

ssh k8s-node1 dnf repolist
repo id                          repo name
appstream                        Rocky Linux 10 - AppStream
baseos                           Rocky Linux 10 - BaseOS
extras                           Rocky Linux 10 - Extras
offline-repo                     Offline repo for kubespray

ssh k8s-node1 cat /etc/yum.repos.d/offline.repo
[offline-repo]
baseurl = http://192.168.10.10/rpms/local
enabled = 1
gpgcheck = 0
name = Offline repo for kubespray

# 기존 레포를 비활성화, 설정하지 않을 경우 kubespray 실패
for i in rocky-addons rocky-devel rocky-extras rocky; do
  ssh k8s-node1 "mv /etc/yum.repos.d/$i.repo /etc/yum.repos.d/$i.repo.original"
  ssh k8s-node2 "mv /etc/yum.repos.d/$i.repo /etc/yum.repos.d/$i.repo.original"
done

ssh k8s-node1 tree /etc/yum.repos.d/
ssh k8s-node1 dnf repolist

ssh k8s-node2 tree /etc/yum.repos.d/
ssh k8s-node2 dnf repolist

which kubectl
/usr/bin/which: no kubectl in (/root/.venv/3.12/bin:/root/.local/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin)

# group vars 
# 권한 변경, cni 변경, kube-proxy 모드 변경등
echo "enable_dns_autoscaler: false" >> inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml
echo "kubectl_localhost: true" >> inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml 
echo "flannel_interface: enp0s9" >> inventory/mycluster/group_vars/k8s_cluster/k8s-net-flannel.yml

sed -i 's|kube_owner: kube|kube_owner: root|g' inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml
sed -i 's|kube_network_plugin: calico|kube_network_plugin: flannel|g' inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml
sed -i 's|kube_proxy_mode: ipvs|kube_proxy_mode: iptables|g' inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml
sed -i 's|enable_nodelocaldns: true|enable_nodelocaldns: false|g' inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml
grep -iE 'kube_owner|kube_network_plugin:|kube_proxy_mode|enable_nodelocaldns:' inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml

kube_owner: root
kube_network_plugin: flannel
kube_proxy_mode: iptables
enable_nodelocaldns: false
flannel_interface: enp0s9
metrics_server_enabled: true

grep "^[^#]" inventory/mycluster/group_vars/k8s_cluster/k8s-net-flannel.yml

sed -i 's|helm_enabled: false|helm_enabled: true|g' inventory/mycluster/group_vars/k8s_cluster/addons.yml
sed -i 's|metrics_server_enabled: false|metrics_server_enabled: true|g' inventory/mycluster/group_vars/k8s_cluster/addons.yml
grep -iE 'metrics_server_enabled:' inventory/mycluster/group_vars/k8s_cluster/addons.yml
echo "metrics_server_requests_cpu: 25m"     >> inventory/mycluster/group_vars/k8s_cluster/addons.yml
echo "metrics_server_requests_memory: 16Mi" >> inventory/mycluster/group_vars/k8s_cluster/addons.yml

cat roles/kubespray_defaults/vars/main/checksums.yml | grep -i kube -A40 | grep 1.34.3 -B2
kubelet_checksums:
  arm64:
    1.34.3: sha256:765b740e3ad9c590852652a2623424ec60e2dddce2c6280d7f042f56c8c98619
    1.32.0: sha256:bda9b2324c96693b38c41ecea051bab4c7c434be5683050b5e19025b50dbc0bf
  amd64:
    1.34.3: sha256:0e759f40bbc717c05227ae3994b77786f58f59ffa0137a34958c6b26fa5bcbbd
kubectl_checksums:
  arm:
    1.34.3: sha256:e0cf1eddede6abfd539e30ccbb4e50f65b2d6ff44b3bb9d9107ea8775a90a7e4
    1.32.0: sha256:6b33ea8c80f785fb07be4d021301199ae9ee4f8d7ea037a8ae544d5a7514684e
  arm64:
    1.34.3: sha256:46913a7aa0327f6cc2e1cc2775d53c4a2af5e52f7fd8dacbfbfd098e757f19e9
kubeadm_checksums:
  arm64:
    1.34.3: sha256:697cf3aa54f1a5740b883a3b18a5d051b4032fd68ba89af626781a43ec9bccc3
    1.32.0: sha256:5da9746a449a3b8a8312b6dd8c48dcb861036cf394306cfbc66a298ba1e8fbde
  amd64:
    1.34.3: sha256:f9ce265434d306e59d800b26f3049b8430ba71f815947f4bacdcdc33359417fb

# etcd가 amd64 아키로 하드코딩되어있어 이를 수정한다.
cat inventory/mycluster/group_vars/all/offline.yml | grep amd64
etcd_download_url: "{{ files_repo }}/kubernetes/etcd/etcd-v{{ etcd_version }}-linux-amd64.tar.gz"

sed -i 's/amd64/arm64/g' inventory/mycluster/group_vars/all/offline.yml

# 배포
ansible-playbook -i inventory/mycluster/inventory.ini -v cluster.yml -e kube_version="1.34.3"

# dns 관리를 네트워크매니저가 하지 않아 노드에서 서비스명으로 질의가 불가하다.
ssh k8s-node2 cat /etc/NetworkManager/conf.d/dns.conf
[global-dns-domain-*]
servers = 10.233.0.3,192.168.10.10
[global-dns]
searches = default.svc.cluster.local,svc.cluster.local
options = ndots:2,timeout:2,attempts:2

# 기존에는 coredns 설정이 추가되었따.
ssh k8s-node2 cat /etc/resolv.conf
nameserver 192.168.10.10

# 특정 nic을 네트워크 매니저가 관리하지 않는 것을 확인할수 잇다.
ssh k8s-node2 cat /etc/NetworkManager/conf.d/k8s.conf
[keyfile]
unmanaged-devices+=interface-name:kube-ipvs0;interface-name:nodelocaldns

# kubectl 다운로드 확인
file inventory/mycluster/artifacts/kubectl
inventory/mycluster/artifacts/kubectl: ELF 64-bit LSB executable, ARM aarch64, version 1 (SYSV), statically linked, BuildID[sha1]=638a428e8c66d4b9ab3e9f21c1d934c4fe12d447, stripped

#
ls -l inventory/mycluster/artifacts/kubectl
-rwxr-xr-x. 1 root root 58130616 Feb 14 19:22 inventory/mycluster/artifacts/kubectl

tree inventory/mycluster/
inventory/mycluster/
├── artifacts
│   └── kubectl
├── credentials
│   └── kubeadm_certificate_key.creds

cp inventory/mycluster/artifacts/kubectl /usr/local/bin/

# 버전 확인 
kubectl version --client=true
Client Version: v1.34.3
Kustomize Version: v5.7.1

# kubeconfig 복사
mkdir /root/.kube
scp k8s-node1:/root/.kube/config /root/.kube/
sed -i 's/127.0.0.1/192.168.10.11/g' /root/.kube/config

k9s

# 자동완성
source <(kubectl completion bash)
alias k=kubectl
complete -F __start_kubectl k
echo 'source <(kubectl completion bash)' >> /etc/profile
echo 'alias k=kubectl' >> /etc/profile
echo 'complete -F __start_kubectl k' >> /etc/profile

# 이미지 주소 확인 
kubectl get deploy,sts,ds -n kube-system -owide
NAME                             READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS       IMAGES                                                     SELECTOR
deployment.apps/coredns          2/2     2            2           146m   coredns          192.168.10.10:35000/coredns/coredns:v1.12.1                k8s-app=kube-dns
deployment.apps/metrics-server   1/1     1            1           145m   metrics-server   192.168.10.10:35000/metrics-server/metrics-server:v0.8.0   app.kubernetes.io/name=metrics-server,version=0.8.0

NAME                                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE    CONTAINERS     IMAGES                                        SELECTOR
daemonset.apps/kube-flannel              0         0         0       0            0           <none>                   146m   kube-flannel   192.168.10.10:35000/flannel/flannel:v0.27.3   app=flannel
daemonset.apps/kube-flannel-ds-arm       0         0         0       0            0           <none>                   146m   kube-flannel   192.168.10.10:35000/flannel/flannel:v0.27.3   app=flannel
daemonset.apps/kube-flannel-ds-arm64     2         2         2       2            2           <none>                   146m   kube-flannel   192.168.10.10:35000/flannel/flannel:v0.27.3   app=flannel
daemonset.apps/kube-flannel-ds-ppc64le   0         0         0       0            0           <none>                   146m   kube-flannel   192.168.10.10:35000/flannel/flannel:v0.27.3   app=flannel
daemonset.apps/kube-flannel-ds-s390x     0         0         0       0            0           <none>                   146m   kube-flannel   192.168.10.10:35000/flannel/flannel:v0.27.3   app=flannel
daemonset.apps/kube-proxy                2         2         2       2            2           kubernetes.io/os=linux   146m   kube-proxy     192.168.10.10:35000/kube-proxy:v1.34.3        k8s-app=kube-proxy

ssh root@k8s-node1

crictl images
IMAGE                                               TAG                 IMAGE ID            SIZE
192.168.10.10:35000/coredns/coredns                 v1.12.1             138784d87c9c5       73.2MB
192.168.10.10:35000/flannel/flannel-cni-plugin      v1.7.1-flannel1     e5bf9679ea8c3       11.4MB
192.168.10.10:35000/flannel/flannel                 v0.27.3             cadcae92e6360       102MB
192.168.10.10:35000/kube-apiserver                  v1.34.3             cf65ae6c8f700       84.8MB
192.168.10.10:35000/kube-controller-manager         v1.34.3             7ada8ff13e54b       72.6MB
192.168.10.10:35000/kube-proxy                      v1.34.3             4461daf6b6af8       75.9MB
192.168.10.10:35000/kube-scheduler                  v1.34.3             2f2aa21d34d2d       51.6MB
192.168.10.10:35000/metrics-server/metrics-server   v0.8.0              bc6c1e09a843d       80.8MB
192.168.10.10:35000/pause                           3.10.1              d7b100cd9a77b       517kB

tree /etc/containerd/
/etc/containerd/
├── certs.d
│   └── 192.168.10.10:35000
│       └── hosts.toml
├── config.toml
└── cri-base.json

3 directories, 3 files

cat /etc/containerd/config.toml
  [plugins."io.containerd.cri.v1.images"]
    snapshotter = "overlayfs"
    image_pull_progress_timeout = "5m"
  [plugins."io.containerd.cri.v1.images".pinned_images]
    sandbox = "192.168.10.10:35000/pause:3.10.1"
  [plugins."io.containerd.cri.v1.images".registry]
    config_path = "/etc/containerd/certs.d"

cat /etc/containerd/certs.d/192.168.10.10\:35000/hosts.toml
server = "https://192.168.10.10:35000"
[host."http://192.168.10.10:35000"]
  capabilities = ["pull","resolve"]
  skip_verify = true
  override_path = false
```

### k8s 참조 이미지 레지스트리 설정
```sh
# k8s-node 외부 통신 불가
ping -c 1 -w 1 -W 1 8.8.8.8
1 packets transmitted, 0 received, 100% packet loss, time 0ms

# 라우팅 확인
ip route
default via 192.168.10.10 dev enp0s9 proto static metric 200 
10.233.64.0/24 dev cni0 proto kernel scope link src 10.233.64.1 
10.233.65.0/24 via 10.233.65.0 dev flannel.1 onlink 
192.168.10.0/24 dev enp0s9 proto kernel scope link src 192.168.10.11 metric 100 

crictl images
IMAGE                                               TAG                 IMAGE ID            SIZE
192.168.10.10:35000/coredns/coredns                 v1.12.1             138784d87c9c5       73.2MB
192.168.10.10:35000/flannel/flannel-cni-plugin      v1.7.1-flannel1     e5bf9679ea8c3       11.4MB
192.168.10.10:35000/flannel/flannel                 v0.27.3             cadcae92e6360       102MB
192.168.10.10:35000/kube-apiserver                  v1.34.3             cf65ae6c8f700       84.8MB
192.168.10.10:35000/kube-controller-manager         v1.34.3             7ada8ff13e54b       72.6MB
192.168.10.10:35000/kube-proxy                      v1.34.3             4461daf6b6af8       75.9MB
192.168.10.10:35000/kube-scheduler                  v1.34.3             2f2aa21d34d2d       51.6MB
192.168.10.10:35000/metrics-server/metrics-server   v0.8.0              bc6c1e09a843d       80.8MB
192.168.10.10:35000/pause                           3.10.1              d7b100cd9a77b       517kB

tree /etc/containerd/certs.d/
/etc/containerd/certs.d/
└── 192.168.10.10:35000
    └── hosts.toml
2 directories, 1 file

# 샘플 앱 배포
cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:alpine   # docker.io/library/nginx:alpine
          ports:
            - containerPort: 80
EOF

# 이미지 당겨오기 실패
# 기본적으로 당겨오는 이미지 경로가 docker.io이기 떄문이다.
kubectl describe pod
Warning  Failed     6s    kubelet            Failed to pull image "nginx:alpine": rpc error: code = DeadlineExceeded desc = failed to pull and unpack image "docker.io/library/nginx:alpine": failed to resolve image: failed to do request: Head "https://registry-1.docker.io/v2/library/nginx/manifests/alpine": dial tcp 35.169.121.184:443: i/o timeout

# admin
# docker 이미지 당겨오기 
podman pull nginx:alpine

podman images | grep nginx
docker.io/library/nginx                                alpine                                                        128568fed7ff  9 days ago     62.9 MB

# 이미지 태그
podman tag nginx:alpine 192.168.10.10:35000/library/nginx:alpine

podman images | grep nginx
docker.io/library/nginx                                alpine                                                        128568fed7ff  9 days ago     62.9 MB
192.168.10.10:35000/library/nginx                      alpine                                                        128568fed7ff  9 days ago     62.9 MB

# https 인증 비활성화
cat <<EOF >> /etc/containers/registries.conf
[[registry]]
location = "192.168.10.10:35000"
insecure = true
EOF

# 설정 확인
grep "^[^#]" /etc/containers/registries.conf
unqualified-search-registries = ["registry.access.redhat.com", "registry.redhat.io", "docker.io"]
short-name-mode = "enforcing"
[[registry]]
location = "192.168.10.10:35000"
insecure = true

podman push 192.168.10.10:35000/library/nginx:alpine

curl -s 192.168.10.10:35000/v2/_catalog | jq |grep nginx
    "ingress-nginx/controller",
    "library/nginx",

curl -s 192.168.10.10:35000/v2/library/nginx/tags/list | jq
{
  "name": "library/nginx",
  "tags": [
    "1.28.0-alpine",
    "1.29.4",
    "alpine"
  ]
}

# 샘플 앱 확인
kubectl get pod
NAME                    READY   STATUS             RESTARTS   AGE
nginx-54fc99c8d-w4zcl   0/1     ImagePullBackOff   0          3m42s

kubectl get deploy -owide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES         SELECTOR
nginx   0/1     1            0           3m53s   nginx        nginx:alpine   app=nginx

# 이미지 변경 
kubectl set image deployment/nginx nginx=192.168.10.10:35000/library/nginx:alpine

# 재확인 
kubectl get deploy -owide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES                                     SELECTOR
nginx   1/1     1            1           4m11s   nginx        192.168.10.10:35000/library/nginx:alpine   app=nginx

# 동작확인
kubectl get pod
NAME                    READY   STATUS    RESTARTS   AGE
nginx-5ff7dd7b8-gw29b   1/1     Running   0          17s

kubectl delete deployments.apps nginx

cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:alpine   # docker.io/library/nginx:alpine
          ports:
            - containerPort: 80
EOF

# 하지만 문제가 해결되지 않기 때문에, 내부적으로 이를 변경해야한다.
kubectl get pod
NAME                    READY   STATUS         RESTARTS   AGE
nginx-54fc99c8d-4kq6z   0/1     ErrImagePull   0          32s

# k8s-node
ssh root@k8s-node1

mkdir -p /etc/containerd/certs.d/docker.io

cat <<EOF > /etc/containerd/certs.d/docker.io/hosts.toml
# 원본 레지스트리
server = "https://docker.io"        
# 내부 내지시트리 지정 즉, docker.io를 대체할 레지스트리 
[host."http://192.168.10.10:35000"]   
  # 이미지 pull, 다이제트스트 해석 허용
  capabilities = ["pull", "resolve"] 
  # https 검증 X
  skip_verify = true                 
EOF

# 재시작
systemctl restart containerd

nerdctl pull docker.io/library/nginx:alpine
elapsed: 0.6 s                                                                    total:  25.2 M (42.0 MiB/s)   

crictl images | grep nginx
192.168.10.10:35000/library/nginx                   1.28.0-alpine       5a91d90f47ddf       51.2MB
192.168.10.10:35000/library/nginx                   alpine              128568fed7ff6       26.4MB
docker.io/library/nginx                             alpine              128568fed7ff6       26.4MB

kubectl get pod
NAME                    READY   STATUS    RESTARTS   AGE
nginx-54fc99c8d-4kq6z   1/1     Running   0          2m52s

# admin
cat /root/kubespray-offline/outputs/kubespray-2.30.0/inventory/mycluster/group_vars/all/offline.yml | head -n 15
http_server: "http://192.168.10.10"
registry_host: "192.168.10.10:35000"

# Insecure registries for containerd
containerd_registries_mirrors:
  - prefix: "{{ registry_host }}"
    mirrors:
      - host: "http://{{ registry_host }}"
        capabilities: ["pull", "resolve"]
        skip_verify: true

# 다음 설정 추가 
vi inventory/mycluster/group_vars/all/offline.yml
# Insecure registries for containerd
containerd_registries_mirrors:
  - prefix: "{{ registry_host }}"
    mirrors:
      - host: "http://{{ registry_host }}"
        capabilities: ["pull", "resolve"]
        skip_verify: true
  - prefix: "docker.io"
    mirrors:
      - host: "http://192.168.10.10:35000"
        capabilities: ["pull", "resolve"]
        skip_verify: false
  - prefix: "registry-1.docker.io"
    mirrors:
      - host: "http://192.168.10.10:35000"
        capabilities: ["pull", "resolve"]
        skip_verify: false
  - prefix: "quay.io"
    mirrors:
      - host: "http://192.168.10.10:35000"
        capabilities: ["pull", "resolve"]
        skip_verify: false

cat inventory/mycluster/group_vars/all/offline.yml | head -n 30

# 설정 업데이트
ansible-playbook -i inventory/mycluster/inventory.ini -v cluster.yml -e kube_version="1.34.3" --tags containerd

ssh k8s-node2 tree /etc/containerd
/etc/containerd
├── certs.d
│   ├── 192.168.10.10:35000
│   │   └── hosts.toml
│   ├── docker.io
│   │   └── hosts.toml
│   ├── quay.io
│   │   └── hosts.toml
│   └── registry-1.docker.io
│       └── hosts.toml
├── config.toml
└── cri-base.json

ssh k8s-node2 cat /etc/containerd/certs.d/quay.io/hosts.toml
quay.io/hosts.toml
server = "https://quay.io"
[host."http://192.168.10.10:35000"]
  capabilities = ["pull","resolve"]
  skip_verify = false
  override_path = false
```

## helm artifact 레포지토리 설정
### Case 0: 수동으로 nginx helm 차트 작성 및 배포
```sh
# admin

podman pull nginx:1.28.0-alpine

# nginx 이미지 확인
podman images | grep nginx
docker.io/library/nginx                                alpine                                                        128568fed7ff  9 days ago     62.9 MB
192.168.10.10:35000/library/nginx                      alpine                                                        128568fed7ff  9 days ago     62.9 MB
docker.io/library/nginx                                1.29.4                                                        85e894eaa91f  11 days ago    184 MB
registry.k8s.io/ingress-nginx/controller               v1.13.3                                                       21bfedf4686d  4 months ago   334 MB
docker.io/library/nginx                                1.28.0-alpine                                                 5a91d90f47dd  9 months ago   51.2 MB

# 태그
podman tag nginx:1.28.0-alpine 192.168.10.10:35000/library/nginx:1.28.0-alpine
192.168.10.10:35000/library/nginx                      1.28.0-alpine                                                 5a91d90f47dd  9 months ago   51.2 MB

# 이미지 푸시
podman push 192.168.10.10:35000/library/nginx:1.28.0-alpine

# 이미지 확인
curl -s 192.168.10.10:35000/v2/library/nginx/tags/list | jq
nx/tags/list | jq
{
  "name": "library/nginx",
  "tags": [
    "1.28.0-alpine",
    "1.29.4",
    "alpine"
  ]
}

cd 
mkdir nginx-chart
cd nginx-chart

# 헬름 템플릿 생성
mkdir templates
cat > templates/configmap.yaml <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}
data:
  index.html: |
{{ .Values.indexHtml | indent 4 }}
EOF

cat > templates/deployment.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app: {{ .Release.Name }}
    spec:
      containers:
      - name: nginx
        image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
        ports:
        - containerPort: 80
        volumeMounts:
        - name: index-html
          mountPath: /usr/share/nginx/html/index.html
          subPath: index.html
      volumes:
      - name: index-html
        configMap:
          name: {{ .Release.Name }}
EOF

cat > templates/service.yaml <<EOF
apiVersion: v1
kind: Service
metadata:
  name: {{ .Release.Name }}
spec:
  selector:
    app: {{ .Release.Name }}
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30000
  type: NodePort
EOF

cat > values.yaml <<EOF
indexHtml: |
  <!DOCTYPE html>
  <html>
  <head>
    <title>Welcome to Nginx!</title>
  </head>
  <body>
    <h1>Hello, Kubernetes!</h1>
    <p>Nginx version 1.28.0 - alpine</p>
  </body>
  </html>

image:
  repository: nginx
  tag: 1.28.0-alpine

replicaCount: 1
EOF

cat > Chart.yaml <<EOF
apiVersion: v2
name: nginx-chart
description: A Helm chart for deploying Nginx with custom index.html
type: application
version: 1.0.0
appVersion: "1.28.0-alpine"
EOF

tree
.
├── Chart.yaml
├── templates
│   ├── configmap.yaml
│   ├── deployment.yaml
│   └── service.yaml
└── values.yaml

2 directories, 5 files

# 렌더링으로 적용값 확인
helm template dev-nginx . -f values.yaml

# 헬름 배포
helm install dev-nginx . -f values.yaml

helm list
NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                APP VERSION  
dev-nginx       default         1               2026-02-14 23:19:07.820924919 +0900 KST deployed        nginx-chart-1.0.0    1.28.0-alpine

kubectl get deploy,svc,ep,cm dev-nginx -owide
Warning: v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                SELECTOR
deployment.apps/dev-nginx   1/1     1            1           20s   nginx        nginx:1.28.0-alpine   app=dev-nginx

NAME                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE   SELECTOR
service/dev-nginx   NodePort   10.233.23.170   <none>        80:30000/TCP   20s   app=dev-nginx

NAME                  ENDPOINTS        AGE
endpoints/dev-nginx   10.233.65.7:80   20s

NAME                  DATA   AGE
configmap/dev-nginx   1      20s

curl http://192.168.10.11:30000
<body>
  <h1>Hello, Kubernetes!</h1>
  <p>Nginx version 1.28.0 - alpine</p>
</body>
...

helm package .
Successfully packaged chart and saved it to: /root/nginx-chart/nginx-chart-1.0.0.tgz

tar -tzf nginx-chart-1.0.0.tgz
nginx-chart/Chart.yaml
nginx-chart/values.yaml
nginx-chart/templates/configmap.yaml
nginx-chart/templates/deployment.yaml
nginx-chart/templates/service.yaml

zcat nginx-chart-1.0.0.tgz | tar -xOf - nginx-chart/Chart.yaml
apiVersion: v2
appVersion: 1.28.0-alpine
description: A Helm chart for deploying Nginx with custom index.html
name: nginx-chart
type: application
version: 1.0.0

zcat nginx-chart-1.0.0.tgz | tar -xOf - nginx-chart/values.yaml
image:
  repository: nginx
  tag: 1.28.0-alpine
...

helm uninstall dev-nginx

helm list

kubectl get deploy,svc,ep,cm dev-nginx -owide
Error from server (NotFound): configmaps "dev-nginx" not found
...
```

### Case 1: 외부 패키킹 다운로드 후 배포
```sh
# admin
podman pull docker.io/bitnami/nginx:latest
podman tag bitnami/nginx:latest 192.168.10.10:35000/bitnami/nginx:latest
podman push 192.168.10.10:35000/bitnami/nginx:latest

# 미사용 
helm repo list

# oci 레지스트리 참조 명령어 
helm show readme oci://registry-1.docker.io/bitnamicharts/nginx
helm show values oci://registry-1.docker.io/bitnamicharts/nginx
helm show chart oci://registry-1.docker.io/bitnamicharts/nginx

cd
mkdir nginx-oci-reg && cd nginx-oci-reg
helm pull oci://registry-1.docker.io/bitnamicharts/nginx --version 22.4.7
 
# 파일 목록 확인
tar -tzf nginx-22.4.7.tgz
nginx/
nginx/charts/
nginx/charts/common/
nginx/charts/common/templates/
nginx/charts/common/templates/validations/
nginx/templates/httproute.yaml

zcat nginx-22.4.7.tgz| tar -xOf - nginx/Chart.yaml
zcat nginx-22.4.7.tgz| tar -xOf - nginx/values.yaml
zcat nginx-22.4.7.tgz| tar -xOf - nginx/values.schema.json

helm install my-nginx ./nginx-22.4.7.tgz --set service.type=NodePort

helm repo list
Error: no repositories to show

helm list
my-nginx        default         1               2026-02-14 23:28:10.757543265 +0900 KST deployed        nginx-22.4.7 1.29.5     

# 메타데이터 확인 시 차트명과 원본 출처의 정보가 출력된다
helm get metadata my-nginx
NAME: my-nginx
CHART: nginx
VERSION: 22.4.7
APP_VERSION: 1.29.5
ANNOTATIONS: fips=true,images=- name: git
  version: 2.53.0
  image: registry-1.docker.io/bitnami/git:latest
- name: nginx
  version: 1.29.5
  image: registry-1.docker.io/bitnami/nginx:latest
- name: nginx-exporter
  version: 1.5.1
  image: registry-1.docker.io/bitnami/nginx-exporter:latest
,licenses=Apache-2.0,tanzuCategory=clusterUtility
DEPENDENCIES: common
NAMESPACE: default
REVISION: 1
STATUS: deployed
DEPLOYED_AT: 2026-02-14T23:28:10+09:00

kubectl get deploy -owide
my-nginx   1/1     1            1           94s   nginx        registry-1.docker.io/bitnami/nginx:latest   app.kubernetes.io/instance=my-nginx,app.kubernetes.io/name=nginx

helm get manifest my-nginx | grep 'image:'
          image: registry-1.docker.io/bitnami/nginx:latest

helm uninstall my-nginx
```

### Case 2-A: 헬름 차트 저장소(ChartMuseum) 활용
```sh
# 저장 디렉토리 
mkdir -p /data/chartmuseum/charts
chmod 777 /data/chartmuseum/charts 

podman run -d \
  --name chartmuseum \
  -p 8080:8080 \
  -v /data/chartmuseum/charts:/charts \
  -e STORAGE=local \
  -e STORAGE_LOCAL_ROOTDIR=/charts \
  -e DEBUG=true \
  ghcr.io/helm/chartmuseum:v0.16.4

podman ps
6b270c982989  ghcr.io/helm/chartmuseum:v0.16.4              1 second ago  Up 1 second  0.0.0.0:8080->8080/tcp  chartmuseum

# 상태 확인
curl -s http://192.168.10.10:8080/health | jq
{
  "healthy": true
}

# 목록 확인
curl -s http://192.168.10.10:8080/api/charts | jq
{}

# 저장소 등록 
helm repo add internal http://192.168.10.10:8080
helm repo update

helm repo list
NAME            URL                      
internal        http://192.168.10.10:8080

# 헬름 푸쉬 플러그인을 활용한 업로드
helm plugin install https://github.com/chartmuseum/helm-push.git

helm plugin list
NAME    VERSION DESCRIPTION                      
cm-push 0.11.1  Push chart package to ChartMuseum

ls -l /root/nginx-chart/*.tgz
-rw-r--r--. 1 root root 854 Feb 14 23:20 /root/nginx-chart/nginx-chart-1.0.0.tgz

helm cm-push /root/nginx-chart/nginx-chart-1.0.0.tgz internal
Pushing nginx-chart-1.0.0.tgz to internal...
Done.

# 갱신
helm repo update

# 차트 확인
curl -s http://192.168.10.10:8080/api/charts | jq
{
  "nginx-chart": [
    {
      "name": "nginx-chart",
      "version": "1.0.0",
      "description": "A Helm chart for deploying Nginx with custom index.html",
      "apiVersion": "v2",
      "appVersion": "1.28.0-alpine",
      "type": "application",
      "urls": [
        "charts/nginx-chart-1.0.0.tgz"
      ],
      "created": "2026-02-14T15:53:17.370276947Z",
      "digest": "68c62f0312a0329bb65f59aa001a5c9e7ed20c41640b0c85badbcc6cae48be04"
    }
  ]
}

# 차트 사용
helm install my-nginx internal/nginx-chart

helm list
NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART              APP VERSION  
my-nginx        default         1               2026-02-15 00:53:59.225965937 +0900 KST deployed        nginx-chart-1.0.0  1.28.0-alpine

kubectl get deploy,svc,ep,cm my-nginx -owide
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                SELECTOR
deployment.apps/my-nginx   1/1     1            1           26s   nginx        nginx:1.28.0-alpine   app=my-nginx
NAME               TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE   SELECTOR
service/my-nginx   NodePort   10.233.4.195   <none>        80:30000/TCP   26s   app=my-nginx
NAME                 ENDPOINTS         AGE
endpoints/my-nginx   10.233.65.10:80   26s
NAME                DATA   AGE
configmap/my-nginx   1      26s

curl http://192.168.10.11:30000
  <h1>Hello, Kubernetes!</h1>
  <p>Nginx version 1.28.0 - alpine</p>

# 삭제
helm uninstall my-nginx

helm list
NAME    NAMESPACE       REVISION        UPDATED STATUS  CHART   APP VERSION

helm repo list
NAME            URL                      
internal        http://192.168.10.10:8080
```

### Case 2-B: 내부망에 사내 OCI 레지스트리에 nginx 차트 업로드 후 사용 
```sh
# 헬름 차트 업로드 
helm push /root/nginx-chart/nginx-chart-1.0.0.tgz oci://192.168.10.10:35000/helm-charts

curl -s 192.168.10.10:35000/v2/_catalog | jq | grep helm
    "helm-charts/nginx-chart",

curl -s 192.168.10.10:35000/v2/helm-charts/nginx-chart/tags/list | jq
{
  "name": "helm-charts/nginx-chart",
  "tags": [
    "1.0.0"
  ]
}

helm install my-nginx oci://192.168.10.10:35000/helm-charts/nginx-chart --version 1.0.0

helm list
my-nginx        default         1               2026-02-15 00:48:24.212269989 +0900 KST deployed        nginx-chart-1.0.0  1.28.0-alpine

kubectl get deploy,svc,ep,cm my-nginx -owide

curl http://192.168.10.11:30000
  <h1>Hello, Kubernetes!</h1>
  <p>Nginx version 1.28.0 - alpine</p>
...

# 삭제
helm uninstall my-nginx

helm list
NAME    NAMESPACE       REVISION        UPDATED STATUS  CHART   APP VERSION
```

## dnf 레포지토리 
```sh
# admin
# 외부 패키지 저장소 동기화. 로컬 디렉토리로 가져오는 역할을 수행한다.
mkdir -p /root/kubespray-offline/outputs/rpms/rocky/10
cd /root/kubespray-offline/outputs/rpms/rocky/10

tree /etc/yum.repos.d/
/etc/yum.repos.d/
├── offline.repo
├── rocky-addons.repo.original
├── rocky-devel.repo.original
├── rocky-extras.repo.original
└── rocky.repo.original

1 directory, 9 files

# 비활성화 풀기 
for i in rocky-addons rocky-devel rocky-extras rocky; do
  mv /etc/yum.repos.d/$i.repo.original /etc/yum.repos.d/$i.repo
done

dnf repolist
repo id                                         repo name
appstream                                       Rocky Linux 10 - AppStream
baseos                                          Rocky Linux 10 - BaseOS
extras                                          Rocky Linux 10 - Extras
offline-repo                                    Offline repo

# 특정 레포(extras, baseos, appstream) 동기화
# --download-metadata 옵션으로 원본 메타데이터 가져오기 그래야만 dnf가 인식할 수 있다.
dnf reposync --repoid=extras --download-metadata -p /root/kubespray-offline/outputs/rpms/rocky/10
dnf reposync --repoid=baseos --download-metadata -p /root/kubespray-offline/outputs/rpms/rocky/10
dnf reposync --repoid=appstream --download-metadata -p /root/kubespray-offline/outputs/rpms/rocky/10

# 가용 용량 확인
df -hT /
Filesystem     Type  Size  Used Avail Use% Mounted on
/dev/sda3      xfs   116G   59G   58G  51% /

free -h
               total        used        free      shared  buff/cache   available
Mem:           1.8Gi       566Mi        89Mi       7.2Mi       1.2Gi       1.2Gi
Swap:          3.8Gi        53Mi       3.8Gi

curl http://192.168.10.10/rpms/rocky/10/
open http://192.168.10.10/rpms/rocky/10/baseos/

# k8s-node
# k8s 노드에서 admin 서버로 dnf 레포지토리를 참조하도록 설정
ssh root@k8s-node1

tree /etc/yum.repos.d
/etc/yum.repos.d
├── offline.repo
├── rocky-addons.repo.original
├── rocky-devel.repo.original
├── rocky-extras.repo.original
└── rocky.repo.original
1 directory, 5 files

# 레포지토리 내부 참조 설정
cat <<EOF > /etc/yum.repos.d/internal-rocky.repo
[internal-baseos]
name=Internal Rocky 10 BaseOS
baseurl=http://192.168.10.10/rpms/rocky/10/baseos
enabled=1
gpgcheck=0

[internal-appstream]
name=Internal Rocky 10 AppStream
baseurl=http://192.168.10.10/rpms/rocky/10/appstream
enabled=1
gpgcheck=0

[internal-extras]
name=Internal Rocky 10 Extras
baseurl=http://192.168.10.10/rpms/rocky/10/extras
enabled=1
gpgcheck=0
EOF

tree /etc/yum.repos.d
/etc/yum.repos.d
├── internal-rocky.repo
├── offline.repo
├── rocky-addons.repo.original
├── rocky-devel.repo.original
├── rocky-extras.repo.original
└── rocky.repo.original

1 directory, 6 files

# 캐시 초기화 후 캐시 업데이트
dnf clean all
dnf repolist
repo id                                   repo name
internal-appstream                        Internal Rocky 10 AppStream
internal-baseos                           Internal Rocky 10 BaseOS
internal-extras                           Internal Rocky 10 Extras
offline-repo                              Offline repo for kubespray

dnf makecache

# 패키지 인스톨 확인 
dnf install -y nfs-utils vim

dnf info nfs-utils | grep -i repo
Repository   : @System
From repo    : internal-baseos
```

## Python 패키지 인덱스 미러 
```sh
# k8s-node
# pypi 인덱스 확인
curl http://192.168.10.10/pypi/
    <a href="ansible/index.html">ansible</a>
    <a href="ansible-core/index.html">ansible-core</a>
    <a href="cffi/index.html">cffi</a>
    <a href="cryptography/index.html">cryptography</a>
    <a href="cython/index.html">Cython</a>
    <a href="distro/index.html">distro</a>
    <a href="flit-core/index.html">flit_core</a>
    <a href="jinja2/index.html">Jinja2</a>
    <a href="jmespath/index.html">jmespath</a>
    <a href="markupsafe/index.html">MarkupSafe</a>
    <a href="netaddr/index.html">netaddr</a>
    <a href="packaging/index.html">packaging</a>
    <a href="pip/index.html">pip</a>
    <a href="pycparser/index.html">pycparser</a>
    <a href="pyyaml/index.html">PyYAML</a>
    <a href="resolvelib/index.html">resolvelib</a>
    <a href="ruamel-yaml/index.html">ruamel.yaml</a>
    <a href="selinux/index.html">selinux</a>
    <a href="setuptools/index.html">setuptools</a>
    <a href="wheel/index.html">wheel</a>

cat /etc/pip.conf
cat <<EOF > /etc/pip.conf
[global]
index-url = http://192.168.10.10/pypi
trusted-host = 192.168.10.10
timeout = 60
EOF

pip list | grep -i netaddr

# etadd 설치
pip install netaddr

# 설치 성공
pip list | grep -i netaddr
netaddr                   1.3.0

# 하지만 admin이 가지고 있지 않은 패키지라면 실패한다
pip install httpx

# admin
cat /root/.config/pip/pip.conf
[global]
index = http://localhost/pypi/
index-url = http://localhost/pypi/
trusted-host = localhost

mv /root/.config/pip/pip.conf /root/.config/pip/pip.bak

# 설치 성공
pip install httpx

pip list | grep httpx
httpx              0.28.1

# whl 파일 조회
find / -name *.whl | tee whl.list

cat whl.list | grep -i http
/root/.cache/pip/wheels/c6/69/46/5e87f24c4c35735a0015d9b6c234048dd71c273d789dffa96f/httpx-0.28.1-py3-none-any.whl
/root/.cache/pip/wheels/f8/71/af/5218b9ba7341d8042b777fa22da7cf0c8e38800049c1e1c2b7/httpcore-1.0.9-py3-none-any.whl

tree /root/kubespray-offline/outputs/pypi/files/
├── ansible-10.7.0-py3-none-any.whl
├── ansible-10.7.0.tar.gz
├── ansible_core-2.17.14-py3-none-any.whl
├── ansible_core-2.17.14.tar.gz
├── cffi-2.0.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.whl
├── cffi-2.0.0-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.whl
├── cffi-2.0.0.tar.gz
├── 
...

# pypi에 httpx 패키지 추가
cp /root/.cache/pip/wheels/c6/69/46/5e87f24c4c35735a0015d9b6c234048dd71c273d789dffa96f/httpx-0.28.1-py3-none-any.whl /root/kubespray-offline/outputs/pypi/files/
cp /root/.cache/pip/wheels/e2/97/87/1d50df62e089aa3562d353120c0a2beb322e5a66a37dc8dffc/anyio-4.12.1-py3-none-any.whl /root/kubespray-offline/outputs/pypi/files/

tree /root/kubespray-offline/outputs/pypi/files/

cd /root/kubespray-offline/
./pypi-mirror.sh

curl http://192.168.10.10/pypi/
    <a href="httpx/index.html">httpx</a>

# 다시 k8s-node에 접속 후 설치 시도 
# 가능은 하나 여러 의존성들로 인해 어려움
ssh root@k8s-node1
pip install httpx
```

[^1]: https://github.com/kubespray-offline/kubespray-offline
[^2]: https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/offline